{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NetTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))                               \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Two CNN + two FC Layers NN\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # use nn.Sequential() for each layer1 and layer2, with nn.Conv2d + nn.ReLU + nn.MaxPool2d\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =1, out_channels =3, kernel_size=(2,2)), # output dimension = (28-2+1)*27*3= 2187\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=1) # output dimension = (27-2+1)*26*3 = \n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =3, out_channels =9, kernel_size=(2,2)), # output dimension = (26-2+1)*25*9\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=1) #output dimension = (25-2+1)*24*9 = 5184\n",
    "        )\n",
    "        # use nn.Linear with 1000 neurons\n",
    "        self.fc1 = nn.Linear(5184, 1000)\n",
    "        # use nn.Linear to output a one hot vector to encode the output\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # use reshape() to match the input of the FC layer1\n",
    "        out = out.reshape(x.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        # use F.log_softmax() to normalize the output\n",
    "        sm = nn.functional.log_softmax(out, _stacklevel=3)\n",
    "        return sm\n",
    "\n",
    "# Another model inspired from LeNet\n",
    "class Model_Type_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Type_LeNet, self).__init__()\n",
    "        # use nn.Sequential() for each layer1 and layer2, with nn.Conv2d + nn.ReLU + nn.MaxPool2d\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =1, out_channels =6, kernel_size=(5,5)), # output dimension = (28-5+1)*24*6= 2187\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2,2), stride=2) # output dimension = ((24-2)%2+1)*12*6 = \n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =6, out_channels =16, kernel_size=(5,5)), # output dimension = (12-5+1)*8*16\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2,2), stride=2) #output dimension = ((8-2)%2+1)*4*16 = 256\n",
    "        )\n",
    "        # use nn.Linear with 1000 neurons\n",
    "        self.fc1 = nn.Linear(256, 1000)\n",
    "        # use nn.Linear to output a one hot vector to encode the output\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # print(out.size())\n",
    "        # use reshape() to match the input of the FC layer1\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        # use F.log_softmax() to normalize the output\n",
    "        sm = nn.functional.log_softmax(out, _stacklevel=3)\n",
    "        return sm\n",
    "    \n",
    "    \n",
    "model_lenet = Model_Type_LeNet() # model type LeNet is not good for this applciation. Maybe it's because of the dimensions of the input picture or the MaxPool2d instead of the MeanPool2D\n",
    "\n",
    "model_lenet.to(device)\n",
    "# use an optim.Adam() optimizer\n",
    "optimizer = optim.Adam(model_lenet.parameters(),  lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9494528/9912422 [00:01<00:00, 1133133.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9920512it [00:31, 310944.16it/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/28881 [00:10<?, ?it/s]\u001b[A\n",
      "0it [00:00, ?it/s]192/28881 [00:10<00:24, 837.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 532480/1648877 [00:00<00:03, 345467.35it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=256\n",
    "\n",
    "train_loader = dataloader.DataLoader(\n",
    "    MNIST('./data', train=True, download=True, transform=NetTransform), \n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_loader = dataloader.DataLoader(\n",
    "    MNIST('./data', train=False, download=True, transform=NetTransform), \n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:53, 30804.50it/s] \n",
      "8192it [00:52, 155.04it/s]              \n",
      "C:\\Users\\User\\Anaconda3\\envs\\Python\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/10 [7936/60000 (12%)]\tLoss: 0.327134"
     ]
    }
   ],
   "source": [
    "from functions.training import train_model, evaluate_model\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "EPOCHS = 10\n",
    "batch_size = 256\n",
    "nb_batch = int(np.ceil(len(train_loader.dataset)/batch_size))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  losses.extend(train_model(train_loader, device, model_lenet, optimizer, epoch, losses, EPOCHS, batch_size)[-nb_batch:])\n",
    "  accuracies.extend(evaluate_model(model_lenet, device, test_loader, losses[-1], EPOCHS, epoch, batch_size, len(train_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
